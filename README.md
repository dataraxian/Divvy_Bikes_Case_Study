# ğŸš² Divvy Bikes Data Pipeline (S3 â†’ DuckDB)

A modular, testable, and reproducible ETL pipeline designed to ingest **Divvy bike trip data** from a public **S3 bucket**, process and store it locally using **DuckDB**, and support downstream analytics or dashboarding.

---

## ğŸ“ Project Structure

```
project-root/
â”œâ”€â”€ s3_divvy/              # Core logic (modular ETL pipeline)
â”‚   â”œâ”€â”€ config.py          # Config paths, flags, and constants
â”‚   â”œâ”€â”€ core.py            # S3 listing, downloads, extraction, hashing
â”‚   â”œâ”€â”€ metadata.py        # Metadata comparison + saving/loading
â”‚   â”œâ”€â”€ processing.py      # CSV ingestion (pandas / DuckDB)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.py    # Pipeline entrypoint script
â”‚
â”œâ”€â”€ data/                  # Local data directories
â”‚   â”œâ”€â”€ zip/               # Downloaded ZIP files
â”‚   â”œâ”€â”€ csv/               # Extracted CSVs
â”‚   â””â”€â”€ hash/              # SHA256 hashes of files
â”‚
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ file_metadata.csv  # Current S3 state snapshot
â”‚
â”œâ”€â”€ tests/                 # Unit + integration tests (pytest)
â”‚   â”œâ”€â”€ test_core.py
â”‚   â”œâ”€â”€ test_metadata.py
â”‚   â”œâ”€â”€ test_processing.py
â”‚   â””â”€â”€ test_run_pipeline.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ environment.yml
â”œâ”€â”€ pytest.ini
â””â”€â”€ README.md
```

---

## ğŸ§  Features

âœ… Public S3 integration (HTTPS or boto3-optional)  
âœ… Delta-aware metadata comparison (new + updated files only)  
âœ… Safe file hashing (SHA-256) for reproducibility  
âœ… Flexible processing backend: `pandas` or `duckdb`  
âœ… Hybrid + bulk DuckDB ingestion modes  
âœ… PyTest suite with full unit + integration coverage  
âœ… Configurable, extensible structure for real-world scale

---

## ğŸš€ Quickstart

### 1. ğŸ“¦ Setup the environment (via conda)
```bash
conda env create -f environment.yml
conda activate s3-divvy
```

### 2. âœ… Run tests
```bash
pytest -v
```

### 3. ğŸƒ Run the pipeline
```bash
python scripts/run_pipeline.py        # Default: duckdb mode
python scripts/run_pipeline.py bulk   # Optional: bulk mode
```

---

## âš™ï¸ Modes

### `duckdb` (default)
- Processes one file at a time
- Loads each into its own table
- Appends to a unified `trips` table (with `source_file` column)

### `bulk`
- Loads all CSVs using DuckDBâ€™s `read_csv_auto()` with `union_by_name=True`
- Much faster for full refresh scenarios

---

## ğŸ“¦ Tech Stack

- **Python 3.12**
- **DuckDB** â€“ Local SQL engine for fast analytics
- **Pandas** â€“ Metadata and fallback processing
- **Boto3 / Requests** â€“ S3 listing and file downloads
- **PyTest** â€“ Testing framework
- **Moto** â€“ AWS mocking for tests

---

## ğŸ“Š Portfolio Use Case
- Designed to support **Power BI**, **Jupyter**, or **Streamlit** dashboards
- Can easily export from DuckDB â†’ `.parquet`, `.csv`, or API-ready formats
- Fully local, portable, and reproducible

---

## ğŸ“ˆ Example Extensions
- â“ Add CLI flags to control date filtering or destination
- ğŸ“Š Build a Power BI dashboard on top of `trips` table
- â˜ï¸ Export to cloud warehouse (e.g., BigQuery or Snowflake)
- ğŸ§ª Add GitHub Actions for CI

---

## ğŸ‘¤ Author
Jonathan [your name or GitHub]  
Built as part of a data analytics engineering portfolio âš¡

---

## ğŸ“„ License
MIT or your choice
